# 高度前処理パイプライン設計書

## 概要

AIファクトチェッカーの精度向上のため、Webページの出力を効果的に整える高度な前処理（Preprocessing）パイプラインを設計します。このパイプラインは、情報収集とembeddingの間に配置され、知識ベースの質を劇的に向上させることを目的としています。

## 現在の実装状況

### 既存の処理フロー
1. **scraper.py**: 基本的なWebスクレイピング
   - BeautifulSoupを使用したHTML解析
   - 基本的な不要タグの除去（script, style, header, footer, nav, aside, form）
   - 単純なテキスト抽出

2. **text_processor.py**: 基本的なテキスト処理
   - 固定サイズでのチャンキング（文字数ベース）
   - NLTK使用の文分割

### 現在の課題
- HTML構造の詳細な解析が不十分
- テキストクリーニングが基本的すぎる
- 意味的なまとまりを考慮しないチャンキング
- Webページ特有のノイズ（広告、ナビゲーション等）の除去が不完全

## 提案する高度前処理パイプライン

### アーキテクチャ概要

```
Webページ → 本文抽出 → テキストクリーニング → 意味的チャンキング → Embedding
```

### 1. 本文抽出（Content Extraction）

#### 1.1 高度なHTML構造解析
- **主要コンテンツ識別**
  - `<main>`, `<article>`, `<section>`タグの優先的抽出
  - CSSセレクタベースの本文領域特定
  - コンテンツ密度分析による本文エリア推定

- **ノイズ除去の強化**
  - 広告ブロック（`class`や`id`に"ad", "advertisement", "banner"を含む要素）
  - ナビゲーション要素（`<nav>`, `.navigation`, `.menu`）
  - サイドバー（`.sidebar`, `.aside`, `<aside>`）
  - フッター・ヘッダー（`.header`, `.footer`, `<header>`, `<footer>`）
  - コメントセクション（`.comments`, `.comment-section`）
  - SNSボタン・共有ボタン（`.share`, `.social`）

#### 1.2 ライブラリ活用
- **trafilatura**: 高精度な本文抽出
- **newspaper3k**: ニュース記事特化の抽出
- **readability-lxml**: Mozillaのreadabilityアルゴリズム実装

#### 1.3 実装例
```python
import trafilatura
from newspaper import Article
from readability import Document

def advanced_content_extraction(url: str, html_content: str) -> str:
    """
    複数の手法を組み合わせた高度な本文抽出
    """
    # Method 1: trafilatura
    trafilatura_text = trafilatura.extract(html_content)
    
    # Method 2: newspaper3k
    article = Article(url)
    article.set_html(html_content)
    article.parse()
    newspaper_text = article.text
    
    # Method 3: readability
    doc = Document(html_content)
    readability_text = doc.summary()
    
    # 最適な結果を選択（長さと品質のバランス）
    return select_best_extraction(trafilatura_text, newspaper_text, readability_text)
```

### 2. テキストクリーニング（Text Cleaning）

#### 2.1 基本的なクリーニング
- **空白・改行の正規化**
  - 連続する空白の単一化
  - 不要な改行の除去
  - タブ文字の統一

- **HTMLエンティティのデコード**
  - `&amp;` → `&`
  - `&lt;` → `<`
  - `&gt;` → `>`
  - `&quot;` → `"`

#### 2.2 高度なクリーニング
- **残存HTMLタグの除去**
  - 正規表現による完全なタグ除去
  - 属性付きタグの処理

- **特殊文字・記号の処理**
  - Unicode正規化（NFKC）
  - 不可視文字の除去
  - 絵文字の適切な処理

- **言語固有の処理**
  - 日本語：全角・半角の統一
  - 英語：縮約形の展開（don't → do not）

#### 2.3 実装例
```python
import re
import html
import unicodedata

def advanced_text_cleaning(text: str) -> str:
    """
    高度なテキストクリーニング処理
    """
    # HTMLエンティティのデコード
    text = html.unescape(text)
    
    # 残存HTMLタグの除去
    text = re.sub(r'<[^>]+>', '', text)
    
    # Unicode正規化
    text = unicodedata.normalize('NFKC', text)
    
    # 連続する空白の正規化
    text = re.sub(r'\s+', ' ', text)
    
    # 不要な改行の除去
    text = re.sub(r'\n\s*\n', '\n', text)
    
    # 先頭・末尾の空白除去
    text = text.strip()
    
    return text
```

### 3. 意味的チャンキング（Semantic Chunking）

#### 3.1 文境界の認識
- **高精度文分割**
  - spaCyによる言語モデルベース分割
  - 略語・固有名詞を考慮した分割
  - 引用符内の句読点の適切な処理

#### 3.2 段落・セクション認識
- **構造的分割**
  - 段落境界の検出
  - 見出し構造の活用
  - リスト項目の適切な処理

#### 3.3 意味的まとまりの保持
- **トピック境界検出**
  - 文間の意味的類似度計算
  - トピック変化点の自動検出
  - 文脈の連続性を保つチャンク分割

#### 3.4 実装例
```python
import spacy
from sentence_transformers import SentenceTransformer
import numpy as np

class SemanticChunker:
    def __init__(self):
        self.nlp = spacy.load("ja_core_news_sm")
        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def semantic_chunk(self, text: str, max_chunk_size: int = 500) -> List[str]:
        """
        意味的まとまりを考慮したチャンキング
        """
        # 文分割
        doc = self.nlp(text)
        sentences = [sent.text for sent in doc.sents]
        
        # 文の埋め込み表現を計算
        embeddings = self.sentence_model.encode(sentences)
        
        # 意味的類似度に基づくチャンク分割
        chunks = []
        current_chunk = []
        current_size = 0
        
        for i, sentence in enumerate(sentences):
            # 文を現在のチャンクに追加
            current_chunk.append(sentence)
            current_size += len(sentence)
            
            # チャンクサイズ制限チェック
            if current_size >= max_chunk_size:
                # 意味的境界を探す
                if i < len(sentences) - 1:
                    similarity = np.dot(embeddings[i], embeddings[i+1])
                    if similarity < 0.7:  # 閾値以下なら分割
                        chunks.append(' '.join(current_chunk))
                        current_chunk = []
                        current_size = 0
        
        # 残りのチャンクを追加
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
```

## パイプライン統合設計

### 新しいモジュール構成

#### preprocessing_pipeline.py
```python
from typing import List, Dict, Any
import trafilatura
from newspaper import Article
import spacy
from sentence_transformers import SentenceTransformer

class AdvancedPreprocessingPipeline:
    """
    高度前処理パイプラインのメインクラス
    """
    
    def __init__(self):
        self.nlp = spacy.load("ja_core_news_sm")
        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def process_url(self, url: str) -> List[Dict[str, Any]]:
        """
        URLから高品質なチャンクを生成する完全なパイプライン
        """
        # 1. Webページの取得
        html_content = self._fetch_webpage(url)
        
        # 2. 本文抽出
        clean_text = self._extract_content(url, html_content)
        
        # 3. テキストクリーニング
        cleaned_text = self._clean_text(clean_text)
        
        # 4. 意味的チャンキング
        chunks = self._semantic_chunk(cleaned_text)
        
        # 5. チャンク品質評価・フィルタリング
        quality_chunks = self._filter_quality_chunks(chunks)
        
        return [{"source": url, "text": chunk} for chunk in quality_chunks]
```

### 既存システムとの統合

#### main.pyの修正点
```python
# 従来
from scraper import extract_text_from_urls
from text_processor import chunk_text

documents = extract_text_from_urls(reference_urls)
all_chunks = []
for doc in documents:
    chunks = chunk_text(doc, chunk_size=500, chunk_overlap=50)
    all_chunks.extend(chunks)

# 新システム
from preprocessing_pipeline import AdvancedPreprocessingPipeline

pipeline = AdvancedPreprocessingPipeline()
all_chunks = []
for url in reference_urls:
    chunks = pipeline.process_url(url)
    all_chunks.extend(chunks)
```

## 品質評価・最適化

### チャンク品質指標
1. **情報密度**: 実質的な情報量 / 総文字数
2. **完整性**: 文の切断がないか
3. **関連性**: トピックの一貫性
4. **可読性**: 人間が理解しやすいか

### パフォーマンス最適化
- **並列処理**: 複数URLの同時処理
- **キャッシュ機能**: 処理済みコンテンツの保存
- **段階的処理**: 軽量な前処理から重い処理へ

## 期待される効果

### 定量的改善
- **ノイズ除去率**: 80%以上のノイズ削減
- **情報密度**: 2-3倍の向上
- **チャンク品質**: 意味的一貫性の大幅向上

### 定性的改善
- **ファクトチェック精度**: より正確な証拠文書の特定
- **処理速度**: 高品質データによる効率的な検索
- **ユーザー体験**: より信頼性の高い結果

## 実装ロードマップ

### Phase 1: 基盤構築（1-2週間）
- [ ] trafilatura, newspaper3k, spaCyの導入
- [ ] 基本的な前処理パイプラインの実装
- [ ] 既存システムとの統合テスト

### Phase 2: 高度化（2-3週間）
- [ ] 意味的チャンキングの実装
- [ ] 品質評価システムの構築
- [ ] パフォーマンス最適化

### Phase 3: 評価・改善（1週間）
- [ ] A/Bテストによる効果測定
- [ ] ファインチューニング
- [ ] ドキュメント整備

## 技術的依存関係

### 新規ライブラリ
```
trafilatura>=1.6.0
newspaper3k>=0.2.8
readability-lxml>=0.8.1
spacy>=3.4.0
sentence-transformers>=2.2.0
scikit-learn>=1.1.0
```

### 言語モデル
```bash
python -m spacy download ja_core_news_sm
```

## 結論

この高度前処理パイプラインの導入により、AIファクトチェッカーの知識ベース品質が大幅に向上し、結果としてファクトチェック全体の精度向上が期待できます。段階的な実装により、リスクを最小化しながら確実な改善を実現できます。
