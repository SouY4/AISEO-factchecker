import os
from dotenv import load_dotenv
from openai import OpenAI
from cohere import Client as CohereClient

# å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰å¿…è¦ãªé–¢æ•°/ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from scraper import extract_text_from_urls
from text_processor import chunk_text, split_into_sentences
from knowledge_base import KnowledgeBase
from verifier import FactChecker

def main():
    """
    ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã®å…¨ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°ã€‚
    """
    # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
    load_dotenv()

    # --- 1. åˆæœŸè¨­å®š ---
    print("Initializing API clients...")
    try:
        openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        cohere_client = CohereClient(api_key=os.getenv("COHERE_API_KEY"))
    except Exception as e:
        print(f"Error initializing API clients. Make sure your .env file is set up correctly. Error: {e}")
        return

    # --- 2. å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®å®šç¾© ---
    # ã“ã“ã«æ¤œè¨¼ã—ãŸã„è¨˜äº‹ã¨å‚è€ƒæ–‡çŒ®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„
    article_to_verify = """
    å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã¯ã€è‡ªç„¶è¨€èªå‡¦ç†ã®åˆ†é‡ã§é©å‘½ã‚’ã‚‚ãŸã‚‰ã—ã¾ã—ãŸã€‚
    ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€æ•°åå„„ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¡ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¸Šã®è†¨å¤§ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ã¾ã™ã€‚
    ãã®çµæœã€äººé–“ã®ã‚ˆã†ãªæ–‡ç« ã‚’ç”Ÿæˆã—ãŸã‚Šã€è³ªå•ã«ç­”ãˆãŸã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ãŸã‚Šã™ã‚‹èƒ½åŠ›ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚
    ã—ã‹ã—ã€LLMã¯æ™‚ã¨ã—ã¦äº‹å®Ÿã«åŸºã¥ã‹ãªã„æƒ…å ±ã‚’ç”Ÿæˆã™ã‚‹ã€Œãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã€ã¨ã„ã†èª²é¡Œã‚‚æŠ±ãˆã¦ã„ã¾ã™ã€‚
    ã“ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€RAGï¼ˆRetrieval-Augmented Generationï¼‰ã¨ã„ã†æŠ€è¡“ãŒæ³¨ç›®ã•ã‚Œã¦ã„ã¾ã™ã€‚
    """
    
    reference_urls = [
        "https://ja.wikipedia.org/wiki/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB",
        "https://ja.wikipedia.org/wiki/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92" # å°‘ã—é–¢é€£æ€§ã®ä½ã„è¨˜äº‹ã‚’æ··ãœã‚‹
    ]

    # --- 3. å‹•çš„çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ ---
    print("\n--- Building Knowledge Base from References ---")
    documents = extract_text_from_urls(reference_urls)
    
    all_chunks = []
    for doc in documents:
        chunks = chunk_text(doc, chunk_size=500, chunk_overlap=50)
        all_chunks.extend(chunks)
        
    knowledge_base = KnowledgeBase(openai_client)
    
    # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ã‚’è©¦è¡Œã—ã€å¤±æ•—ã—ãŸå ´åˆã¯é©åˆ‡ã«ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã™ã‚‹
    try:
        knowledge_base.build(all_chunks)
        if knowledge_base.table is None:
            print("âš ï¸  çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            print("ã“ã‚Œã¯ä»¥ä¸‹ã®ç†ç”±ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ï¼š")
            print("1. OpenAI APIã®ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã«é”ã—ã¦ã„ã‚‹")
            print("2. APIã‚­ãƒ¼ãŒç„¡åŠ¹ã¾ãŸã¯è¨­å®šã•ã‚Œã¦ã„ãªã„")
            print("3. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã®å•é¡Œ")
            print("\nè§£æ±ºç­–ï¼š")
            print("- OpenAI APIã®èª²é‡‘ãƒ—ãƒ©ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            print("- .envãƒ•ã‚¡ã‚¤ãƒ«ã®OPENAI_API_KEYãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
            print("- ã—ã°ã‚‰ãæ™‚é–“ã‚’ç½®ã„ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„")
            return
    except Exception as e:
        print(f"âš ï¸  çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        if "429" in str(e) or "quota" in str(e).lower():
            print("\nğŸ’¡ ã“ã‚Œã¯OpenAI APIã®ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ã§ã™ã€‚")
            print("è§£æ±ºç­–ï¼š")
            print("1. OpenAIã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®èª²é‡‘æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            print("2. ä½¿ç”¨é‡åˆ¶é™ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦ãƒ—ãƒ©ãƒ³ã‚’ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
            print("3. æœˆã®ä½¿ç”¨é‡ãŒãƒªã‚»ãƒƒãƒˆã•ã‚Œã‚‹ã¾ã§å¾…ã¤ã‹ã€ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
        return

    # --- 4. è¨˜äº‹ã®å„æ–‡ã‚’æ¤œè¨¼ ---
    print("\n--- Verifying Each Sentence of the Article ---")
    sentences = split_into_sentences(article_to_verify)
    fact_checker = FactChecker(openai_client, cohere_client)
    
    final_results = []
    for sentence in sentences:
        if not sentence.strip():  # ç©ºã®æ–‡ã¯ã‚¹ã‚­ãƒƒãƒ—
            continue
        result = fact_checker.verify_sentence(sentence, knowledge_base)
        final_results.append(result)

    # --- 5. æœ€çµ‚çµæœã®è¡¨ç¤º ---
    print("\n\n==========================================")
    print("          Fact-Checking Report")
    print("==========================================")
    for result in final_results:
        print(f"\n[Sentence] {result.get('original_sentence', 'N/A')}")
        print(f"  - Score: {result.get('score', 'N/A')}/100")
        print(f"  - Decision: {result.get('decision', 'N/A')}")
        print(f"  - Reason: {result.get('reason', 'N/A')}")
        
        # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã®è©³ç´°è¡¨ç¤º
        if 'score' in result and result['score'] is not None:
            similarity_score = result['score'] / 100.0  # 0-1ã®ç¯„å›²ã«å¤‰æ›
            print(f"  - Similarity Score: {similarity_score:.3f} (Cosine similarity between sentence and top evidence)")
        
        # å‚ç…§å…ƒæ–‡ç« ã®è©³ç´°è¡¨ç¤º
        if 'evidence' in result and result['evidence']:
            print(f"  - Top Evidence Source: {result['evidence'][0]['source']}")
            print(f"  - Top Evidence Text: \"{result['evidence'][0]['text'][:200]}...\"")  # æœ€åˆã®200æ–‡å­—ã‚’è¡¨ç¤º
            
            # è¿½åŠ ã®è¨¼æ‹ ãŒã‚ã‚Œã°è¡¨ç¤º
            if len(result['evidence']) > 1:
                print(f"  - Additional Evidence Sources ({len(result['evidence'])-1} more):")
                for i, evidence in enumerate(result['evidence'][1:], 2):
                    print(f"    {i}. {evidence['source']}")
                    print(f"       \"{evidence['text'][:150]}...\"")  # æœ€åˆã®150æ–‡å­—ã‚’è¡¨ç¤º


if __name__ == "__main__":
    main()
